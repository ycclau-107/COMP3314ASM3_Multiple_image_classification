{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AUwbFEaMiWi"
   },
   "source": [
    "# COMP3314 - Assignment 2\n",
    "\n",
    "## Question 2: Spam classifier (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T18:21:06.553305Z",
     "start_time": "2022-10-26T18:21:06.548561Z"
    },
    "id": "pCXQMN-Hus1Y"
   },
   "source": [
    "### Step 1: Download dataset\n",
    "Download examples of spam and ham from Apache SpamAssassinâ€™s public datasets. Split the datasets into a training set and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:05:59.155299Z",
     "start_time": "2022-10-27T05:05:58.015019Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbs5g3YbuH59",
    "outputId": "5013e61d-6017-4d01-ca89-5a1c86094816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples: 2436\n",
      "The number of test samples: 610\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import shutil\n",
    "import sklearn.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def download_dataset():\n",
    "\n",
    "    def download_url(url, dataset_dir=\"data\"):\n",
    "\n",
    "        tar_dir = os.path.join(dataset_dir, \"tar\")\n",
    "        if not os.path.isdir(tar_dir):\n",
    "            os.makedirs(tar_dir)\n",
    "\n",
    "        filename = url.rsplit(\"/\", 1)[-1]\n",
    "        tarpath = os.path.join(tar_dir, filename)\n",
    "\n",
    "        try:\n",
    "            tarfile.open(tarpath)\n",
    "        except:\n",
    "            urlretrieve(url, tarpath)\n",
    "\n",
    "        with tarfile.open(tarpath) as tar:\n",
    "            dirname = os.path.join(dataset_dir, tar.getnames()[0])\n",
    "            if os.path.isdir(dirname):\n",
    "                shutil.rmtree(dirname)\n",
    "            tar.extractall(path=dataset_dir)\n",
    "\n",
    "            cmds_path = os.path.join(dirname, \"cmds\")\n",
    "            if os.path.isfile(cmds_path):\n",
    "                os.remove(cmds_path)\n",
    "\n",
    "        return dirname\n",
    "\n",
    "    def load_dataset(dirpath):\n",
    "        files = []\n",
    "        filepaths = glob.glob(dirpath + \"/*\")\n",
    "        for path in filepaths:\n",
    "            with open(path, \"rb\") as f:\n",
    "                byte_content = f.read()\n",
    "                str_content = byte_content.decode(\"utf-8\", errors=\"ignore\")\n",
    "                files.append(str_content)\n",
    "        return files\n",
    "\n",
    "    spam_url = \"https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\"\n",
    "    easy_ham_url = \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\"\n",
    "    hard_ham_dir = \"https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\"\n",
    "\n",
    "    spam = load_dataset(download_url(spam_url))\n",
    "    easy_ham = load_dataset(download_url(easy_ham_url))\n",
    "    hard_ham = load_dataset(download_url(hard_ham_dir))\n",
    "\n",
    "    X = spam + easy_ham + hard_ham\n",
    "    y = np.concatenate((\n",
    "        np.ones(len(spam)),\n",
    "        np.zeros(len(easy_ham) + len(hard_ham)),\n",
    "    ))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Download dataset.\n",
    "X, y = download_dataset()\n",
    "\n",
    "# Split dataset into training and testing sets.\n",
    "X, y = sklearn.utils.shuffle(X, y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(f\"The number of training samples: {len(X_train)}\")\n",
    "print(f\"The number of test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4RYzHc0vsyi"
   },
   "source": [
    "### Step 2: Feature extraction (5 points)\n",
    "\n",
    "Next, we are going to do some data cleaning and feature extraction.\n",
    "\n",
    "1. Some data cleaning functions have been provided to you. You'll need to implement `lower_letters()`, `convert_num_to_word()`, and `remove_punctuation()`. These functions will convert email to lowercase, replace all numbers with \"NUM\", and remove punctuation.\n",
    "2. Convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector that indicates the presence or absence of each possible word. For example, if all emails only ever contain four words, \"Hello,\" \"how,\" \"are,\" \"you,\" then the email \"Hello you Hello Hello you\" would be converted into a vector [1, 0, 0, 1] (meaning [\"Hello\" is present, \"how\" is absent, \"are\" is absent, \"you\" is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word. You may check sklearn's `CountVectorizer` class for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:05:59.258405Z",
     "start_time": "2022-10-27T05:05:59.252825Z"
    },
    "id": "fegetO2HwHBo"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class EmailCleaner(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,\n",
    "                 no_header=True,\n",
    "                 to_lowercase=True,\n",
    "                 url_to_word=True,\n",
    "                 num_to_word=True,\n",
    "                 remove_punc=True,\n",
    "                 ):\n",
    "        self.no_header = no_header\n",
    "        self.to_lowercase = to_lowercase\n",
    "        self.url_to_word = url_to_word\n",
    "        self.num_to_word = num_to_word\n",
    "        self.remove_punc = remove_punc\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_cleaned = []\n",
    "        for email in X:\n",
    "            if self.no_header:\n",
    "                email = EmailCleaner.remove_header(email)\n",
    "            if self.to_lowercase:\n",
    "                email = EmailCleaner.lower_letters(email)\n",
    "\n",
    "            email_words = email.split()\n",
    "            if self.url_to_word:\n",
    "                email_words = EmailCleaner.convert_url_to_word(email_words)\n",
    "            if self.num_to_word:\n",
    "                email_words = EmailCleaner.convert_num_to_word(email_words)\n",
    "            email = \" \".join(email_words)\n",
    "            if self.remove_punc:\n",
    "                email = EmailCleaner.remove_punctuation(email)\n",
    "            X_cleaned.append(email)\n",
    "        return X_cleaned\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_header(email):\n",
    "        return email[email.index(\"\\n\\n\"):]\n",
    "\n",
    "    @staticmethod\n",
    "    def is_url(s):\n",
    "        url = re.match(\n",
    "            \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|\"\n",
    "            \"[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", s)\n",
    "        return url is not None\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_url_to_word(words):\n",
    "        for i, word in enumerate(words):\n",
    "            if EmailCleaner.is_url(word):\n",
    "                words[i] = \"URL\"\n",
    "        return words\n",
    "\n",
    "    @staticmethod\n",
    "    def lower_letters(email):\n",
    "        # === Your code here ===\n",
    "        email = email.lower()\n",
    "        return email\n",
    "        # ======================\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_num_to_word(words):\n",
    "        # === Your code here ===\n",
    "        for i, word in enumerate(words):\n",
    "            if word.isdigit():\n",
    "                words[i] = \"NUM\"\n",
    "        return words\n",
    "        # ======================\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(email):\n",
    "        # === Your code here ===\n",
    "        email = email.translate(str.maketrans('','',string.punctuation))\n",
    "        return email\n",
    "        # ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:00.474628Z",
     "start_time": "2022-10-27T05:06:00.471273Z"
    },
    "id": "tLVTFnZius1e"
   },
   "outputs": [],
   "source": [
    "# Here are some unit tests to check your code.\n",
    "# Your code should at least pass the following tests.\n",
    "# Feel free to add more tests if you\"d like.\n",
    "\n",
    "# Check lower_letters().\n",
    "src_string = \"Message-Id: <LISTMANAGERSQL-25343\"\n",
    "dst_string = \"message-id: <listmanagersql-25343\"\n",
    "assert EmailCleaner.lower_letters(src_string) == dst_string\n",
    "\n",
    "# Check convert_num_to_word().\n",
    "src_string = \"Date: Wed, 10 Jul 2002\"\n",
    "src_words = src_string.split()\n",
    "dst_words = [\"Date:\", \"Wed,\", \"NUM\", \"Jul\", \"NUM\"]\n",
    "assert EmailCleaner.convert_num_to_word(src_words) == dst_words\n",
    "\n",
    "# Check remove_punctuation().\n",
    "src_string = \"superstars -- you'll find investing more fun...\"\n",
    "dst_string = \"superstars  youll find investing more fun\"\n",
    "assert EmailCleaner.remove_punctuation(src_string) == dst_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:02.761239Z",
     "start_time": "2022-10-27T05:06:01.254913Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPZFUkLAus1f",
    "outputId": "5739ee2e-188b-487e-c560-9722c37fce15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2436, 108540)\n",
      "(610, 108540)\n"
     ]
    }
   ],
   "source": [
    "# Step 1 of pipeline: data cleaning.\n",
    "email_cleaner = EmailCleaner()\n",
    "\n",
    "# Step 2 of pipeline: CountVectorizer.\n",
    "# === Your code here ===\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "# ======================\n",
    "\n",
    "# Build pipeline.\n",
    "prepare_pipeline = Pipeline([\n",
    "    (\"email_cleaner\", email_cleaner),\n",
    "    (\"count_vectorizer\", count_vectorizer),\n",
    "])\n",
    "\n",
    "# Run preprocessing.\n",
    "X_all = X_train + X_test\n",
    "prepare_pipeline.fit(X_all)\n",
    "X_all = prepare_pipeline.transform(X_all)\n",
    "num_train = len(X_train)\n",
    "X_train = X_all[:num_train]\n",
    "X_test = X_all[num_train:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZYuW1qnvth8"
   },
   "source": [
    "### Step 3: Train a spam classifier (5 points)\n",
    "\n",
    "Next, let's build a spam classifier, and train your classifier with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:25.443785Z",
     "start_time": "2022-10-27T05:06:25.420443Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjkN2tXeus1g",
    "outputId": "ca789153-a45f-4c6d-d8f1-784f0116eb22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(random_state=0, solver='liblinear'),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'C': [1, 10.0, 100.0, 1000.0, 10000.0, 100000.0,\n",
       "                               1000000.0, 10000000.0, 100000000.0, 1000000000.0,\n",
       "                               10000000000.0]},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Your code here ===\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "tfidf.fit_transform(X_train)\n",
    "np.set_printoptions(precision=4)\n",
    "X_train_tfid = tfidf.transform(X_train)\n",
    "X_test_tfid = tfidf.transform(X_test)\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(estimator = LogisticRegression(random_state = 0, solver = 'liblinear'),\n",
    "             param_grid={'C': [1,1e1,1e2,1e3,1e4,1e5,1e6,1e7,1e8,1e9,1e10]},\n",
    "             scoring  = 'accuracy',\n",
    "             cv = 5,\n",
    "             verbose = 2,\n",
    "             n_jobs = -1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train_tfid,y_train)\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcgEbSMHbbay",
    "outputId": "cb0784e0-4184-400d-96f0-9216bc5cbcce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'C': 100000.0} \n",
      "CV Accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' %gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2ceMI4Kus1g"
   },
   "source": [
    "### Step 4: Eval your classifier\n",
    "\n",
    "Test your classifier with the test set and print the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:26.371331Z",
     "start_time": "2022-10-27T05:06:26.364811Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itmgIWW6us1h",
    "outputId": "675e2393-e7c8-4cd6-e030-a366482d4f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:\n",
      "Precision: 0.9857 \tRecall: 0.9821\n"
     ]
    }
   ],
   "source": [
    "# === Your code here ===\n",
    "from sklearn.metrics import *\n",
    "def printPrecisionRecall(label, ys_test, ys_test_hat, ys_train = None, ys_train_hat = None,command = 1):\n",
    "    print(\"%s:\" %label)\n",
    "    # command 1 : for step(3)\n",
    "    if(command == 1):\n",
    "        print(\"Precision: %.4f \\t\" % precision_score(ys_test,ys_test_hat), end = '')\n",
    "        print(\"Recall: %.4f\" % recall_score(ys_test, ys_test_hat))\n",
    "    # command 2 : for step(5)\n",
    "    elif(command == 2):\n",
    "        print(\"Training| Precision: %.4f \\t\" % precision_score(ys_train,ys_train_hat), end = '')\n",
    "        print(\"Recall: %.4f\" % recall_score(ys_test, ys_test_hat))\n",
    "        print(\"Testing | Precision: %.4f \\t\" % precision_score(ys_test,ys_test_hat), end = '')\n",
    "        print(\"Recall: %.4f\" % recall_score(ys_test,ys_test_hat))\n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "printPrecisionRecall(\"LogisticRegression\", y_test, gs_lr_tfidf.predict(X_test_tfid))\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTbxtjvHohqJ"
   },
   "source": [
    "### Step 5: Ensemble of classifiers (5 points)\n",
    "\n",
    "1. Implement 4 new classifiers (in total you have 5 claassifiers now).\n",
    "2. Use hard or soft voting to ensemble thoses classifiers.\n",
    "3. Train your ensemble model on the training set. Report training/testing precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:29.164535Z",
     "start_time": "2022-10-27T05:06:28.530051Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwS_cYldop6_",
    "outputId": "357a6882-7946-4440-faa5-b328bce28006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLR:\n",
      "Training| Precision: 1.0000 \tRecall: 0.9821\n",
      "Testing | Precision: 0.9857 \tRecall: 0.9821\n",
      "DecisionTree:\n",
      "Training| Precision: 0.9701 \tRecall: 0.9214\n",
      "Testing | Precision: 0.9416 \tRecall: 0.9214\n",
      "KNN:\n",
      "Training| Precision: 1.0000 \tRecall: 0.9464\n",
      "Testing | Precision: 0.9431 \tRecall: 0.9464\n",
      "SVM:\n",
      "Training| Precision: 0.9982 \tRecall: 0.9643\n",
      "Testing | Precision: 0.9747 \tRecall: 0.9643\n",
      "RandomForest:\n",
      "Training| Precision: 1.0000 \tRecall: 0.9821\n",
      "Testing | Precision: 0.9786 \tRecall: 0.9821\n",
      "Majority voting:\n",
      "Training| Precision: 1.0000 \tRecall: 0.9821\n",
      "Testing | Precision: 0.9821 \tRecall: 0.9821\n"
     ]
    }
   ],
   "source": [
    "# === Your code here ===\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator\n",
    "class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classifiers, vote='classlabel', weights=None):\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for key, value in _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.vote not in ('probability', 'classlabel'):\n",
    "            raise ValueError(\"vote must be 'probability' or 'classlabel' ; got (vote=%r)\" % self.vote)\n",
    "        if self.weights and len(self.weights) != len(self.classifiers):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d classifiers' % (len(self.weights), len(self.classifiers)))\n",
    "        self.lablenc_ = LabelEncoder()\n",
    "        self.lablenc_.fit(y)       \n",
    "        self.classes_ = self.lablenc_.classes_\n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.vote == 'probability':\n",
    "            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n",
    "        else:\n",
    "            predictions = np.asarray([clf.predict(X) for clf in self.classifiers_]).T\n",
    "            maj_vote = np.apply_along_axis(\n",
    "                       lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions)\n",
    "        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n",
    "        return maj_vote\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probas = np.asarray([clf.predict_proba(X) for clf in self.classifiers_])\n",
    "        avg_proba = np.average(probas, axis=0, weights=self.weights)\n",
    "        return avg_proba\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "clf1 = LogisticRegression(penalty = 'l2', C = 1e5, solver = 'liblinear', random_state = 0)\n",
    "clf2 = DecisionTreeClassifier(max_depth = 10, criterion = 'entropy',random_state = 0)\n",
    "clf3 = KNeighborsClassifier(n_neighbors = 1, p =2, metric='minkowski')\n",
    "clf4 = SVC()\n",
    "clf5 = RandomForestClassifier()\n",
    "clf_labels = ['SLR', 'DecisionTree', 'KNN','SVM','RandomForest']\n",
    "\n",
    "clf_list = [clf1,clf2,clf3,clf4,clf5]\n",
    "\n",
    "mv_clf = MajorityVoteClassifier(classifiers = clf_list)\n",
    "clf_labels += [\"Majority voting\"]\n",
    "clf_all  = clf_list + [mv_clf]\n",
    "for clf, label in zip(clf_all, clf_labels):\n",
    "    clf.fit(X_train_tfid, y_train)\n",
    "    y_train_hat = clf.predict(X_train_tfid)\n",
    "    y_test_hat = clf.predict(X_test_tfid)\n",
    "    printPrecisionRecall(label = label, \n",
    "                         ys_test = y_test, \n",
    "                         ys_test_hat = y_test_hat,\n",
    "                         ys_train = y_train, \n",
    "                         ys_train_hat = y_train_hat,\n",
    "                         command = 2)\n",
    "\n",
    "# ======================"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "49c48b447b9efd9c07d32c0dec9df5e0c02b4225e51c3003920687e790460610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
